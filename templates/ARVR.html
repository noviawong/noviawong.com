{% include "header.html" %}

<section class="projecttitles">
    <div class="max">
        <h1>Augmented and Virtual Reality Prototyping</h1>
    </div>
</section>

<div class="max">
    <section class="projectcontent">
        <div>
            <button class="accordion">Overview</button>
                <div class="panel">
                     <p><b class="lightblue">Role:</b> UX Designer and Engineer</p>
                <p><b class="lightblue">Skills Used:</b><br>A-Frame<br>AR.js<br>Snapchat Lens Studio<br>Ottifox<br>Adobe Illustrator<br>Phsyical Prototyping for AR/VR<br>User Journey Mapping<br>AR/VR Sketching</p>
                <p><b class="lightblue">Duration:</b> 4 months, each project took approximately 2 weeks</p>
                <p><b class="lightblue">Summary:</b><br><b>Goal</b><br>In two weeks, design and create a digital prototype experience using either AR or VR technology for a specific problem space.<br><br><b>Process</b><br>Given that this was part of the class projects, we learned the UX principles behind designing good AR and VR applications, such as being considerate of users fatigue, limit motion and acceleration to avoid motion sickness, and the ethical concerns with AR/VR. Before even learning about what digital tools I could use to prototype the experiences, I first had to determine whether AR or VR would be a better technology to use for the particular problem space. Then, I sketched out user journey and made some quick sketches regarding the potential interfaces and interactions users would have. The next step was to explore tools that I could use to realize the digital experiences. After assessing different options and digital tools for both AR and VR, I then started loading 3D models and virtual elements onto the digital prototypes. Unfortunately, I could not validate the design with actual users, but I was able to receive feedback regarding the technical aspect of the projects as well as the UX considerations of the project.
<br><br><b>Solution</b><br>There were two projects that were the results of this course: a solution that aims to reduce test anxiety in students and test-takers by taking them to a virtual testing site, and a solution that aims to facilitate trying on different makeup looks before making a commitment into buying the products or finding the right tutorials for the “look” they are going for. Two different digital prototype experiences were the final deliverables of the projects.</p>
         </div>
            <h2>Process</h2>
            <img class="process_img" src="{{ url_for('static', filename='ARVR_process.png') }}" alt="Image describing the design process for AR and VR prototyping.">
            <!--Beginning of first stage of research process-->
            <h1>VR Project: Virtual Test Room</h1>
            <h2>Identifying Problem Statement </h2>
            <p>For this VR project, the entire class was given a domain to work with: education. However, even when given this specific domain, it was still an extremely large design problem space to consider. My team and I started by brainstorming a list of possible problems students and teachers face in the domain of education. Examples include: lack of visualization during history classes, lack of personal interaction in online courses, difficulty in fully grasping the concepts behind programming or coding lessons, and test anxiety. We voted on test anxiety as a problem to tackle.</p>
            <p>Our society nowadays relies heavily on standardized testing to access people’s skills and knowledge. Examples include college entrance exams such as SAT and ACT, as well as graduate program entrance exams such as GRE and LSAT. Many of these standardized testing sessions last for hours, or even days. Test anxiety often happens when test-takers are at the testing sites, or even during the test itself. We have identified several reasons that caused test anxiety, including the fear of failure and lack of preparation. According to learning psychology, a large amount of test anxiety can lead to emotional distress and physical symptoms such as nausea.</p>
            <p>Currently, the most common way to address test anxiety is to take practice tests either on paper of through a computer monitor. This strategy familiarizes students with the types of questions they may encounter on the actual test. However, my team theorized that by allowing the test-takers to experience these mockup tests in the actual testing environment will better prepare them and further reduce their test anxiety. Virtual reality (VR) was chosen because only a virtual environment can simulate the actual testing sites without the users physically going to the sites themselves.</p>
            <p>We decided to simulate this experience because many current mock-up tests allow students to take them at their own room at ease, or with a proctor they are already familiar with. When test-takers do want to check out the testing sites, it is often impossible due to various location constraints, including distance and limited access during day time (e.g., if it is a high school classroom).</p>
            <p>After outlining the problem space, we chose to focus our target user to be students, particularly high school students who are preparing to take the SAT or ACT. This user group was chosen because the SAT and ACT are taken on paper, and has the largest population of test-takers compared to the other tests. As a result, this would be a perfect opportunity for pilot testing the design. Another concern we had was simulating the test on paper vs. on computer screens in a virtual environment; we worried that users would experience user fatigue and nausea if they had to stare at a computer screen in the VR space.</p>
            <p>Next, we conducted a competitive analysis to assess what technology currently exists that we might be able to use as references when creating ours. We found that currently solutions are limited and the closest one we found to what we had in mind was a solution to address stage fright for public speaking.</p>
            <img class="ARVR_img" src="{{ url_for('static', filename='ARVR_VRCompetitive1.png') }}" alt="Competitive analysis for VR prototyping. Image shows a list of different potential competitors, including paper-based mockup tests.">
            <img class="ARVR_img" src="{{ url_for('static', filename='ARVR_VRCompetitive2.png') }}" alt="Competitive analysis for VR prototyping. Image shows what each competitor could do and could not do, with comparison to our solution.">

            <h2>Physical Prototyping</h2>
            <p>Before diving into digital prototyping, it was extremely beneficial to start with physical prototyping. My team and I first outlined a current user journey using post-it notes (i.e., what they currently do to prepare for an exam, and how they react once they are at a testing site). By doing so, it allowed us to understand what the key interactions we needed to have for this design. We then selected several key interactions or elements that we would like to show in our prototype. Unfortunately, given that it was all of our first time prototyping something in the VR space, none of us was confident enough to pick more than two or three interactions or key components to showcase in the digital prototype itself.</p>
            <p>We also did some quick paper sketching before the actual physical prototyping to generate as many ideas of possible. However, given that our goal was to provide a realistic virtual testing site for the users, there wasn’t too much in terms of the virtual space setting that we could sketch out. As a result, we focused more on the little details that would complete the experience, such as the placement of the clock (something that usually triggers anxiety), coughing and distracting noises by other test-takers, and the distraction and intimidating sight of the test proctor.</p>
            <img class="ARVR_img" src="{{ url_for('static', filename='ARVR_VRoverheadSketch.jpeg') }}" alt="Overhead sketch showing how the VR space would look like.">
            <img class="ARVR_img" src="{{ url_for('static', filename='ARVR_VRJourneyMapping.jpg') }}" alt="User journey for VR project.">
            <p>To outline the environment and interactions we had in mind, we used paper, playdough, and a bunch of other props to help with the physical prototyping. For example, to show that a pop-up window will show up in front of the user’s field of vision, we used a clear ruler with a piece of paper taped onto the end of the ruler. In addition, we created an enclosed classroom with folded paper. The most tedious but also the most fun part was likely the making of playdough humans. We wanted to emphasize that the users will still see other “people” taking the test alongside them when they are in the virtual environment. So, we made tiny little playdough humans! Physical prototyping was a quick and easy way for us to visualize what we had in mind, and translating it into something 3D, further ensuring that we as a team had the same shared understanding of the project.</p>
            <p>To me, I believe that it was extremely helpful to test out the physical prototyping before going into digital prototyping In particular, it helped me personally to visualize how the users might see from their point of view. To simulate this, we filmed a video with the physical prototype from the user’s point of view; it started with the “user” (i.e., the phone camera) selecting the test they would like to take, then walking into the virtual classroom, listening to the proctor giving out instructions, and ended with the “user” leaving the virtual space.</p>
            <p>By the end of the physical prototyping phase, we decided to approach the following interaction and virtual components in our final digital prototype:</p>
            <ul>
                <li><b>A 360 classroom environment in virtual reality:</b>users will be able to look around the room, including seeing the proctor, the clock on the side wall, and such. We believe that this passive interaction itself is crucial because it would reduce test-takers’ anxiety if they are more familiar with the physical testing space.</li>
                <li><b>Choose a test to practice with:</b>this interaction takes place before the user enters a virtual testing room to take the test in our VR space. This will determine the type of interactions they have in taking the test, that is, either paper or computer.</li>
                <li><b>Choose a test location:</b>this takes place after choosing the specific test to practice with. It will allow the system to retrieve the actual environment and stitch a 360 photo together. </li>
            </ul>
            <iframe class="demo" src="https://www.youtube.com/embed/LCBVwpU2L24" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


            <h2>Digital Prototyping</h2>
            <p>Since it was my first time creating a digital prototype for a VR design, the first thing I had to do was to explore the existing tools that could help me realize this goal. Together, my team and I found several tools that we could use: Ottifox, A-Frame, Sketch, Photoshop, and Proto.io. Without too much thinking, we opted in for Ottifox because it allowed us to create an interactive VR space simply by dragging and dropping 3D models onto the planes. In my opinion, the application was pretty straightforward and simple to use. Many of the 3D models were already made by other users who are more skilled at 3D modeling; as a result, all we had to do was search for what we wanted and drag it to the prototype, adjust the scale and position to fit our design. However, since it's a drag-and-drop application, there are many limitations that we experienced. For example, when we tried to create actual interactions for users to interact with, the only things that we could adjust were position, scale, and rotation. It didn't seem like we could link the interaction to other components in the environment. On critical challenge that we ran into was when the tool wouldn’t allow us to map directly to the components themselves; to deal with this issue, we displayed the whiteboard behind the text-based selection option and simply moved it forward in the plane to cover up the text. It wasn’t the best, but it showed the possible interactions we could have had if Ottifox allowed us to do what we wanted to do.</p>
            <p>Also, we realized that Ottifox didn’t support scene changing. So while we had the environment set up in the virtual space, we had to place all the interactions in one scene. Since Ottifox is still very new to the VR prototype field (it was released within the last couple of years), I think it's a relatively powerful tool for the initial digital VR prototype, especially if all we want is to show our concept and design digitally. Although there is a lack of documentation for Ottifox, it's a good prototype tool for beginners in VR prototyping.</p>
           <video class="demo" controls>
                <source src="{{ url_for('static', filename='ARVR_VROttifox.mov') }}" alt="Video demo of VR prototype using Ottifox.">
            </video><br>
            <p>Since Ottifox was so limiting in creating an interaction, we decided to see if we could solve this by using A-Frame. We switched our prototyping tool knowing that we would at least have something to showcase our concept. Compared to Ottifox, A-Frame relies more on actual programming. At first, A-Frame seemed very intimidating and complex. But upon exploring A-Frame codes for a little bit, it wasn't so bad anymore, especially because there were a lot of tutorials and documentations online for me to make references to. Similar to Ottifox, there are a lot of assets that we could just copy and paste into my own A-Frame's code, because other A-Frame users have already developed the codes for these specific assets. In my opinion, creating interactions was much easier on A-Frame than on Ottifox. On Ottifox, we would create an "action", but on A-Frame, we would create an "event". While Ottifox was limiting in what interactions mean, A-Frame has much more flexibility when it comes to programming user-VR interaction. For example, I was able to make a box change color, on top of everything else Ottifox could do. The only thing was that it took longer to learn the basics of A-Frame because it's code-based. Given that I have a HTML/CSS background, it wasn't that difficult. Comparing between Ottifox and A-Frame, I believe Ottifox prototype is lower in fidelity but takes less time to learn for a beginner.</p>
            <video class="demo" controls>
                <source src="{{ url_for('static', filename='ARVR_VR_A-Frame_Prototype.mp4') }}" alt="Video demo of VR prototype using A-Frame.">
            </video>
            <p>Since text was so difficult to work with in both Ottifox and A-Frame, my team and I used Sketch to show the other interactions we wanted to include in our prototype. Although Sketch didn’t allow us to show an interactive virtual environment, it allowed us to visualize what the interactions would possibly look like.</p>
            <img class="ARVR_img" src="{{ url_for('static', filename='ARVR_VRSketchPrototype.png') }}" alt="VR prototype background using Sketch.">
            <p>One final thought that I have about VR prototyping is that VR prototyping requires a lot of computing power and graphic processing. Since we all used Macbooks, I think that was another limiting factor. I believe PCs would be much better in terms of handling VR prototyping</p>

            <h2>Reflection</h2>
            <p><b>Physical prototyping:</b> With physical prototyping, if I could do it all over again, I would actually like to simulate the experience in a real classroom. The model we built was very small, and it was difficult to get the camera fit in between the playdough pieces. However, if we had done it in a real classroom, it might be easier to visualize the types of interactions we would like to have. However, physical prototyping was still a very beneficial strategy for me to visualize, understand, and refine the design before digital prototyping.</p>
            <p><b>Digital prototyping:</b>Given that it was my first time prototyping something with Ottifox and A-Frame for VR designs, it wasn’t easy. One thing I found surprisingly challenging was to include text in the VR space. After trying to put text on both Ottifox and A-Frame for  hours, I then realized that text has no weight or thickness in a 3D space. As a result, text is much more difficult to work with than 3D models, things I don’t even know how to make myself! Now that I think about it, it is probably better to limit the amount of text in our VR space instead of trying to simulate the entire testing experience. After reading UX guidelines and VR professionals’ personal experiences, it became apparent to me that text often is one of the main reasons users experience nauseousness when using VR devices. Another thing I learned from digital prototyping was to utilize existing resources. Since I didn’t know how to make my own 3D models, I had to rely on borrowing from other people’s 3D models that are open-source and free. This made the digital prototyping experience rather simple and quick in both Ottifox and A-Frame; since I could simply import the model and either drag and drop the model (in Ottifox), or type one or two lines of code (in A-Frame) to load and position the models.</p>
            <p><b>Team work: </b>It was more difficult to collaborate as a team when creating digital prototypes for a VR experience. We couldn’t really split up the work like we normally would in other group projects. Originally, when we were working with Ottifox, the files weren’t sharable. As a result, we had to come together to work on the prototype itself together at a meeting. After we switched over to A-Frame, it was much easier to collaborate than Ottifox, but it still required a lot of communication to update each other regarding the changes we made. It was a lot of debugging together through group chats. But, it was helpful to do this project as a team, because some of us had more experiences in coding than the others, and were able to guide the rest of us through the entire digital prototyping experience.</p>
            <p><b>If I could do it again...</b>I would spend more time learning how to code in A-Frame instead of Ottifox. While Ottifox was significantly easier to use, A-Frame communicated the design much better. Additionally, I might select a few more interactions to prototype with, including adding sounds such as the instructor’s voices and other test-takers’ small coughing to complete the experience. Finally, I would do a bit more validation with our design before prototyping it. More feedback from actual users would be helpful to even see if this design was a helpful one. </p>

            <h1>AR Project: Try-on Makeup</h1>
            <h2>Identify Problem Statement</h2>
            <p>This time, my team and I had the freedom to choose the domain we wanted to work in. For this project, we wanted to focused on interacting with the real, physical world. Similar to the previous project, we brainstormed a list of problem spaces the people faced, such as problems that people faces when they do grocery shopping (e.g., they don’t know what coupons are available, what recipes they could make with the ingredients, etc), as well as lack of visualization in long term commitment such as getting a new haircut. </p>
            <p>We finally decided to choose the domain of beauty and cosmetic. More specifically, we found that e-commerce has drastically changed the beauty industry, giving consumers the ability to purchase beauty products and cosmetics directly online. However, we believe that this online purchase experience introduces a new risk to customers: customers don’t really know how a particular cosmetic or beauty item would look on their own faces, and they don’t really have a tutorial to follow. For larger commitments such as getting a hair color or buying a new pair of glasses, consumers are even more reluctant in making online purchases and committing to the change before they could visualize the new “look”.</p>
            <p>Based on the problem space, we decided to use augmented reality (AR) for this project, because we wanted the users to be able to visualize their new looks on their faces directly through a camera, and then being able to apply this new look by themselves following a step-by-step tutorial. </p>
            <p>Through our competitive analysis, my team and I found that current technological solutions are limited to users uploading a photo of themselves to a system, then the system applies a filter on their faces. Alternatively, users will use something like Snapchat’s or Facebook’s filters, which directly applies a face mask on their faces. One downside to this technology is that they don’t really get to design their own looks or learn how to apply that to their own faces.</p>
            <img class="ARVR_img" src="{{ url_for('static', filename='ARVR_ARCompetitive1.png') }}" alt="Competitive analysis for AR prototyping. Image shows a list of different potential competitors, including a magic mirror makeup kiosk and Snapchat.">
            <img class="ARVR_img" src="{{ url_for('static', filename='ARVR_ARCompetitive2.png') }}" alt="Competitive analysis for AR prototyping. Image shows what each competitor could do and could not do, with comparison to our solution.">
            <p>As a result, my team and I wanted to create an AR experiences that allows users to visualize a new look, then purchasing the cosmetic items online, and following an AR tutorial while actually trying out the look on their own faces. This experience will be triggered by a computer’s camera because we didn’t want the users to hold their mobile phones as they try on different makeup or even try to follow a tutorial.</p>
            <p>The main features we wanted to include at this stage included the following:</p>
            <ul>
                <li>Integrate with existing e-commerce businesses that enable 3D view of fashion and cosmetics products.</li>
                <li>List relevant products that completes the look that user selects, and allow users to change individual product variations to see reflected changes in the final look.</li>
                <li>Provide users with makeup instructions tutorials and products’ information once they choose a specific look.</li>
                <li>Include “Add to shopping cart” feature to streamline purchases. Give users ability to use themselves as models in order to have the most accurate and close-to-life measure of product simulations.</li>
            </ul>


            <h2>Physical Prototyping</h2>
            <p>The moment we started with physical prototyping, my team and I realized that we were way too ambitious with our intended interactions. Although we had experiences prototyping physically and digitally in the VR space at this point, we didn’t have any experiences in AR prototyping. </p>
            <p>Generally speaking, the benefits of physical prototyping was similar to physical prototyping for the VR project; mapping out the user flow before prototyping and sketching out possible interactions and interfaces helped the entire team to visualize the design. However, there were several differences: (1) when physically prototyping for AR designs, we had to consider what lived in the world space (e.g., users’ faces) and what lived in the screen space (e.g., the makeup filters), (2) in AR physical prototyping, we used mostly transparent sheets and paper instead of playdough, and (3) AR prototyping for this project involved a human subject, and we prototyped the experience from a mirror first-person point of view instead of the direct first person point of view.</p>
            <img class="ARVR_img" src="{{ url_for('static', filename='ARVR_ARuserflow.png') }}" alt="User flow for AR project.">
            <p>After physical prototyping, my team and I pretty much limited it to prototyping one interaction instead of trying to tackle many different ones: we decided to focus on prototyping the key interactions:</p>
            <ul>
                <li>Choose an occasion to explore makeup options</li>
                <li>Choose a makeup look to “try on”</li>
                <li>Actually trying on the look</li>
            </ul>
            <video class="demo" controls>
                <source src="{{ url_for('static', filename='ARVR_ARPhysicalPrototype.mp4') }}" alt="Video demo of AR prototype using transparent sheets and paper.">
            </video>

            <h2>Digital Prototype</h2>
            <p>AR digital prototyping was significantly more difficult than VR digital prototyping. Instead of quick drag-and-drop tools like Ottifox, we had to rely on tools such as A-Frame and Unity. Since my team and I have already had some experiences with A-Frame, we decided to continue using A-Frame as a digital prototyping tool.</p>
            <p>Unlike our VR project, it was much more difficult to collaborate as a team to design an AR interface. As a result, my teammates and I all used different digital prototyping tools to prototype this experience. For me, I used a combination of Lens Studio, A-Frame, and Illustrator to create the interaction of selecting a makeup to try on. I first programmed on A-Frame and modified the code given in class in order to use the webcam on my computer. I then used Adobe Illustrator to create different variations of makeup looks users could choose from. At first, I used a 2D lens to simulate what users would be able to see and interact with. However, upon discovering Lens Studio, I created a separate Illustrator file and uploaded the PNG version of that to Lens Studio to create a more dynamic prototype, instead of the 2D, still image.</p>
            <p>To see the details of my A-Frame codes: <a target="_blank" href="https://glitch.com/edit/#!/prickle-catfish?path=index.html:8:17">https://glitch.com/edit/#!/prickle-catfish?path=index.html:8:17</a></p>
            <p>In my digital prototype, the video is a live-feed from the users’ cameras. I used HTML and CSS codes to position to rest of the components on the prototype (e.g., text, makeup looks selection, etc), and then created my own graphics for this prototype. I also created my own face paint for the AR component of this prototype, and used Lens Studio to make it come alive. I created most of the elements that I used in the prototype with the exception of the A-Frame code I used to get the live camera feed from users.</p>
            <p>The major interaction I prototyped with these digital tools was selecting a specific makeup to try on. The video linked above shows that users could ideally select a makeup look that want, and then they would be able to preview it through AR.</p>
            <p>One limitation that I found when I was using A-Frame and AR.js was I didn’t find a good way to link the Lens Studio’s effect directly to A-Frame/AR.js. As a result, I had to show the interactions by stitching two different videos together, instead of using one single digital tool. I expected tools such as the actual Snapchat application and Unity to be more robust than my A-Frame codes.</p>
            <iframe class="demo" src="https://www.youtube.com/embed/NCMi-7I32vc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

            <h2>Reflection</h2>
            <p><b>Physical prototyping:</b> physical prototyping was a playful experience for our group, largely due to the problem statement and domain that we decided to work with. However, thinking back to physical prototyping, I felt more at ease this time around when physical prototyping for AR/VR. One challenge that I didn’t expect to face was camera angle; camera angle was extremely important for AR prototyping, since unlike VR, in AR designs, the camera takes in everything in the real world space. As a result, I learned to consider both the users’ point of view as well as the background context and environment.</p>
            <p><b>Digital prototyping:</b>  a lot of the interactions I wanted to prototype ended being either Wizard of Oz or done through multiple digital tools with post-editing. Compared to VR digital prototype, AR prototyping had a lot to do with the physical world, and I assumed that’s why it was much more difficult to prototype for. I learned that it’s not easy to create a virtual object that interacts with real world components. Often, it requires either a marker tracking, face tracking, or other body movement tracking. With the current technology available for digital prototyping and my own skill level, I would not have been able to produce something higher fidelity.</p>
            <p><b>Problem scoping:</b> when we first started, the idea was to work on the beauty and cosmetic industry as a whole, but we then realized it was way too broad. Just like in traditional interaction design, it is extremely important to set a narrow enough problem scope when designing with constraints. In this case, rather than trying to code all the interactions -- from fashion to hair, from nailart to makeup -- we focused on one single aspect only, and even then, it was still too much given the time limit we had.</p>
            <p><b>Teamwork:</b>I was surprised to learn that my team decided so quickly that the AR project would be an individual effort. However, I think it was nice to have a team to work with regardless, because in the end, I only found out about Snapchat’s Lens Studios through the help of my teammates, and we were able to provide feedback to each other throughout the project.</p>
            <p><b>If I could do it all over again...</b>I would include more interactions in the digital prototype through A-Frame and AR.js. I would also explore other digital tools that may help with the AR prototyping experience. Apparently, one of my teammates recently found TorchAR, which is a new AR prototyping tool that doesn’t rely on coding as heavily as A-Frame or Unity. </p>
    </section>
</div>

{% include "footer.html" %}